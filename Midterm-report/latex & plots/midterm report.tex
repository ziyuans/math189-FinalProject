\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2018

% ready for submission
\usepackage{nips_2018}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

% to compile a preprint version, e.g., for submission to arXiv, add
% add the [preprint] option:
% \usepackage[preprint]{nips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2018}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\title{Yelp Dataset Analysis Report }

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Shihao Lin \\
  Pitzer College\\
  1050 N. Mills. Ave, CA 91711 \\
  \texttt{shilin@pitzer.students.edu} \\
  \\
   \textbf{Ziyuan Shang} \\
   Scripps College\\
   Claremont,CA,91711 \\
   \texttt{zshang9431@scrippscollege.edu} \\
 }


\begin{document}
% \nipsfinalcopy is no longer used

\maketitle
\begin{abstract}
  Rating prediction is one of the most important
application of machine learning. In this project, 
we explore the Yelp dataset and are looking for appropriate machine learning algorithms to predict the star users will assign to a business through the content of review they wrote. Currently, we use the basic topic modeling processed the data and use classification algorithms like Naive Bayes(NB) algorithm, MultinomialNB, BernoulliNB, Logistic Regression and Linear Support Vector Machine to do binary classification (positive and negative) on the dataset. 
\end{abstract}

\section{Introduction}
Yelp is an American multinational corporation headquartered in San Francisco, California. It was founded in 2004, and dedicated to helping people find a great local business like restaurants and hair salons. The company allows users to give star ratings and write reviews for the business. More than 155 millions of reviews have been written by Yelp users by the end of Q1 2018. Among these reviews, myriads of hidden information can be extracted, and this information can be beneficial to business owners as well as customers. By studying these reviews, owners can be more aware of things that they need to improve, and customers will be able to find better matches for their own tastes. However, how to extract the most important information has remained a big challenge for a long time. The project specifically focuses on building a model that can extract useful features from millions of reviews. Processing these reviews using sentiment analysis, finding the most featured words and predicting the proper ratings solely based on review text.

\section{Data description}

Yelp's datasets include six files, which are business.json, review.json, user.json, checkin.json, tip.json, and photos. In our midterm modeling, we used the business.json and review. json. Later, we will further explore the review.json for our final modeling. In the following subsections, we are going to illustrate the datasets and its format.

In this dataset, the total organizations are 146702, the total users are 1326101, the total reviews are 1493480, the total negative reviews (from rating scale 0 to 4) are 504771, and total positive reviews (from rating scale 4 to 5) are 988709. 

\begin{figure}[!ht]
\centering
\includegraphics[width=\textwidth]{ReviewDistribution.png}
\caption{Star Rating Distribution}
\end{figure}

The above figure 1 illustrates the review star distribution. Based on figure 1, we can notice that the total start reviews are mostly made up by positive reviews.

\subsection{Files Description}

The file Business.json contains business data including location data, attributes, and categories. More specific, this file includes information of city, state, latitude, and longitude of the store, stars, review counts, business parking, ambience, price level, Good for meal and etc. In the later section, we would set up the model to illustrate the correlations between all of those attributes and business rating. In other words, we are going to build up a model to predict the business rating applying some of the features to different types of existing algorithms to train the computer.

The file Review.json contains full review text data including the user\_id that wrote the review and the business\_id the review is written for. We will not use this for the midterm report. But we will use this file for the final modeling to predict a solo review star, which is based on its review text.

\section{Data Analysis}
Before we actually start analyzing the data, we transform the format from json to csv, which is convenient for us to analyze the data. In the following subsections, we generate various graphs to help visualize the relationship between different feature.

\subsection{Location v.s Review counts}
In the first part, we link the review counts to states. We find out that Arizona has the most reviews, which is a bit surprising. The topmost reviews states are Arizona, Nevada, Oregon, North Carolina, and Ohio. Surprisingly, we find that California is not on this list. 
\begin{figure}[!ht]
\centering
\includegraphics[width=10cm]{state.png}
\caption{What state has the most reviews?}
\end{figure}
We also link the review counts to the city. We find that Las Vegas has the most reviews, which is 22113 and is also the main component of the total reviews of Nevada. The following figure 2 illustrates the topmost reviews city in the U.S.
\begin{figure}[!ht]
\includegraphics[width=\textwidth]{City.png}
\caption{Which city has the most reviews}
\end{figure}
By using the latitudes and longitudes of business organizations and its review counts, we can plot out a graph about that information, just like the following figure 3 about Las Vegas and Phoenix. Each review will represent a small black dot. The more reviews in the same location, the dot will be darker. Therefore, we can use this graph to predict where is the most popular place in a city.
\begin{figure}[!ht]
\includegraphics[width=\textwidth]{Graph.png}
\caption{Business popularity on city map}
\end{figure}

\subsection{Review Count v.s. rating}

\begin{figure}[t]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{ReviewvR.png}
  \captionof{figure}{Review Count v.s. Rating}
  \label{fig:test1}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Reservation}
  \captionof{figure}{Reservation v.s Rating}
  \label{fig:test2}
\end{minipage}
\end{figure}

In figure 4 "Review count v.s. Rating", we label the number of review in one single business organization into horizontal axis, and the probability to get a positive review into vertical axis. We found that if a business has fewer reviews, the probability of it to be rated as positive is around 50\%. The business organization with a medium amount of review tends to have high variance on its probability to be rated as positive. The organization with large reviews often tend to have the probability to be rated as positive.

\subsection{Reservation v.s. Rating}
We want to discover the relationship between whether an organization accepts reservation would influence it to be rated as positive. Therefore, we analyzed the data and put it into a histogram as presented in figure 5. On the horizontal axis, 0 represents that reservation service is unavailable, and 1 represents that reservation service is available. The vertical axis shows the probability for each type of organization to be rated as positive. As a result, we found the probability of being rate as positive for both types are very similar and centering around 50 percentage. Thus, we conclude that there is no such correlation between the service of reservation and business rating. 

\subsection{Price v.s. Rating}

\begin{figure}[t]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Price.png}
  \captionof{figure}{Price v.s. Rating}
  \label{fig:test1}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Ambience.png}
  \captionof{figure}{Ambience v.s Rating}
  \label{fig:test2}
\end{minipage}
\end{figure}

The price level of a restaurant is always a big factor to influence people's decision of selecting a restaurant. Sometime, people would think an expensive restaurant tend to taste better than a cheap restaurant. In this paper, we are going to present our finding of the correlation between price level and business rating. By plugging the price level data and business rating data into "Seaborn" analyzer, we get a histogram in figure 6. Each number on the horizontal axis represents different price levels. "1.0" means very cheap, and "4.0" means very expensive. The vertical axis shows the probability for different price levels of an organization to be rated as positive. As a result, we found the probability of all four different price level are very similar and close to 50 percentage, and the probability of price level 4 is slightly lower than the price level. Thus, the result indicates the price level may not have much influence on Business Rating. A very expensive restaurant may not always taste very good.

\subsection{Ambience v.s. Rating}

Different type ambience of a restaurant tends to have different chance to be rated as a positive good restaurant. We want to discover the correlation between ambience and rating. In the yelp dataset, it provides 10 categories for ambience, "None", "casual", "divey","intimate","romantic", "trendy","classy",'touristy", "hipster", and "upscale". Then we plugged all of those data with its business rating into "Seaborn" analyzer. We get a bar diagram in figure 7. Different categories are labeled in the horizontal axis. The vertical axis shows the probability for different categories of an organization to be rated as positive. As we observed, "upscale" ambience restaurant tend to have higher chance to be rated as positive. However, in general, since the positive rating probability for all categories is all laid between 40 percentage and 60 percentage, the ambience does not seem to have much correlation with the business rating.

\subsection{City v.s. Rating}
\begin{figure}[t]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{CityR.png}
  \captionof{figure}{City v.s. Rating}
  \label{fig:test1}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Parking.png}
  \captionof{figure}{Parking v.s Rating}
  \label{fig:test2}
\end{minipage}
\end{figure}
We are curious about whether the overall quality of food in some city is higher than other cities. Therefore, we input all organizations' city location and corresponding business rating into "Seaborn" analyzer and get a histogram as shown in figure 8. The cities' names are labeled in a horizontal axis, and the corresponding positive rating probability is presented by a vertical axis. The different color bar represents the different city, and the black bar is the variance of the probability. By observation, we found some city definitely have higher overall quality restaurant than others.

\subsection{Parking v.s. Rating}

The transportation in most of the place in the U.S. is driving. In average, every household has around 2 cars in the U.S. Thus, we think whether a restaurant would provide parking service is essential to its rating. Then we plug the data about its restaurant parking situation and corresponding business rating into "Seaborn" analyzer and get a histogram. As in figure 9, we observe that the positive rating probability for all categories is under 50 percent. Thus, the service of parking does not seem to have much influence on the business rating. 

\subsection{Good For Meal v.s. Rating}
\begin{figure}[t]
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{GFN.png}
	\caption{Good For Meal v.s Rating}
    \end{minipage}
\begin{minipage}{.5\textwidth}
	\centering
    \includegraphics[width=\linewidth]{Algorithm.png}
    \caption{Algorithm.png}
\end{minipage}
\end{figure}
At first, we consider that different meal type may have an influence on the business rating. For example, restaurant, which focuses on making breakfast, in general, may have higher business rating than those focuses on making brunch. In yelp data, it provides 7 categories for "Good For Meal", which are "None", "late night", "dessert", "breakfast", "dinner", "lunch", and "brunch". While we plugged all of those data and corresponding business rating, we get a bar diagram, as shown in figure 10. The meal categories are labeled in a horizontal axis, and the corresponding positive rating probability is presented by a vertical axis. By observation, we found that the positive rating probability for all categories are very similar and around 50 percentage. Thus, distinguishing meal specialty of a restaurant may not help to distinguish its business rating.

\section{Business Analysis}

Before actually applying the data into the algorithm, we decide to filter out some features of the dataset. In the end, we remain the three features, review\_count  Reservation, and Parking, to predict the business rating. We set 75 percentage dataset as the training dataset, and remain 25 percentage dataset as the testing dataset. Then we purposely select six different methods to predict the outcome, which is "Logistic Regression", "Linear Discriminant Analysis", "K-Neighbors Classifier", "Decision Tree Classifier", "Gaussian Naive Bayes", and "Random Forest Classifier" from $sklearn$ package. All of six methods could be used to analyze a binary outcome.

In the previous section, although the features in the dataset are independence to each other, we notice that most of the features in this dataset do not have much correlation with the business rating. Therefore, we assume that the accuracy for those algorithms to have a right prediction is low. The overall result is present in the following table, and the visualized version is in figure 12.

By observation, the overall accuracy is very low and around 50 percentage. We observed that " Logistic regression" method has the lowest variance, "K-nearest Neighbors classifier" has the highest variance, and "Random Forest classifier" brings up the highest accuracy, which is yielding roughly 52.11\%. Although "Decision tree classifier" is the underlying concept for "Random Forest classifier", the accuracy performance does not have much different.   In conclusion, the correlation between the feature and its outcome is essential for generating a high accuracy prediction.

\begin{table}[!ht]
\begin{center}
\begin{tabular}{ |c|c| } 
 \hline
Algorithm & Accuracy Percentage \\ 
  \hline
Logistic\_Regression\_classifier & 0.506369 (0.003203) \\ 
  \hline
Linear\_Discriminant\_Analysis & 0.506685 (0.004129) \\
  \hline
K-Neighbors\_classifier & 0.511014 (0.009426) \\
  \hline
Decision\_Tree\_classifier & 0.520832 (0.006517) \\
  \hline
Gaussian\_Naive\_Bayes & 0.506978 (0.006656) \\
  \hline
Random\_Forest\_classifier & 0.521114 (0.006188) \\
\hline
\end{tabular}
\end{center}
\end{table}

\section{Review Analysis}
In our review analysis, we mainly explored the relationship between review content (what words does user use) and rating of the corresponding business. We first attempted our reviews analysis on a smaller sample. Form our reviews dataset, we generated a random sample of 3000 reviews. We will scale the code to perform analysis for the entire dataset in the future. 

\subsection{Data preparation}
The data used in this analysis comes from the preprocessed review.csv mentioned in Section 3. The only useful columns are \textit{text} - the actual review content given by the user in a string format, and  \textit{review\_stars} - the star rating associated with the review. A third column named \textit{Rating} is created based on \textit{review\_stars} - reviews with 1, 2 or 3 stars are considered negative sentiment; reviews with 4 or 5 stars are considered positive.

\subsubsection{Text pre-processing}
First, we performed several necessary pre-processing on review test, including tokenizing, removing stop words and stemming.

Tokenizing refers to the act of separating a sentence into individual word tokens. In our analysis, we used the default \textit{word\_tokenizer} in the \textit{nltk} package under Python. As an example, if we pass the following sentence:"The python programmer named pythoner is pythoning a game pythonly." into the \textit{word\_tokenizer}, the returned result will be 
\['The','python','programmer', 'named', 'pythoner', 'is', 'pythoning', 'a', 'game', 'pythonly'\] 
After tokenization, we need to remove the common stop words in English. Stop words are words that appear extremely common but do not convey much meaning or sentiment of bodies of text. We used the bad of stop words gathered by the \textit{nltk} package and eliminated the words that falls into this bag from each piece of text. Using the above example, after removing the stop words, the returned result is 
\['The','python','programmer', 'named', 'pythoner', 'pythoning', 'game', 'pythonly'\] 
Observed that the words 'is' and 'a' are removed from the list, whereas the words that conveying meanings are kept. 

Then we stemmed the individual words in each piece of text using Porter Stemmer. Stemming refers to "the process of reducing inflected (or sometimes derived) words to their word stem, base or root from generally a written word form". In particular, Porter Stemmer is the de facto algorithm developed in 1980, and it was widely used to stem English words. In our analysis, we used the \textit{nltk} implementation of Porter Stemmer to bring words from text to its stem form. Here, we assumed that words with the same stem convey roughly the same meaning, and therefore are weighted equally in sentiment analysis. 

After passing the above list to a Porter Stemmer, the list returned is in the following form: 
\['the','python','programm', 'name', 'python', 'python', 'game', 'pythonli'\]
Observed that all python-related words, except 'pythonly', are being turned into its stemmed for 'python'. The single exception of the adverb might be an example of under stemming. 

\subsubsection{Feature selection}
After text processing, another important area that we explored is the feature selection for sentiment prediction. Feature selection for sentiment prediction is a popular topic with many on-going researched. Popular methods for feature selection include information gain and HMM-LDA. For our purposes, we use a rather simplistic feature: individual words.

\subsection{Model training and evaluation}
\subsubsection{Training}
For sentiment predictions, we have tested a few machine learning algorithms that can make binary (positive and negative) classifications using different sets of features. We have explored from the \textit{nltk} package, trained on sparse matrix build by only individual words as features. We also used several more advanced algorithms. Including MNB, BernoulliNB, Logistic Regression, Linear SVC from the \textit{sklearn} package.

\subsubsection{Evaluation}
Below is the graph comparing the accuracy of different algorithms we've tried.   
We can observe that the Naive Bayes Classifier performed poorly, yielding roughly 50.87\%. 
Then, we used several more advanced algorithms, including MNB, BernoulliNB, Logistic Regression, Linear SVC, the test accuracy are as follows: After text processing, another important area that we explored is the feature selection for sentiment prediction. Feature selection for sentiment prediction is a popular topic with many on-going researched. Popular methods for feature selection include information gain and HMM-LDA. For our purposes, we use a rather simplistic feature: individual words.
\begin{table}[!ht]
\begin{center}
\begin{tabular}{ |c|c| } 
 \hline
Classifier & Accuracy Percentage \\ 
  \hline
 MNB\_classifier & 71.8\% \\ 
  \hline
BernoulliNB\_classifier & 64.93\%\\
  \hline
LogisticRegression\_classifier & 73.07\% \\
  \hline
LinearSVC\_classifier & 71.47\% \\
  \hline
\end{tabular}
\end{center}
\end{table}

We see that the selected algorithms are producing results with around 71\% of accuracy. We will use tf-idf and other methods to improve the score in the future. 

\section*{References}
[1] https://www.yelp.com/dataset-challenge.
\end{document}
